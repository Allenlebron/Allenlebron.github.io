---
layout:     post
title:      Hadoop全家桶之编译源码
subtitle:   
date:       2019-05-22
author:     zwilpan
header-img: img/bigdatalearning.jpg
catalog: true
tags:
    - BigData
---

## 为什么hadoop需要重新编译？

	Hadoop是使用Java语言开发的，但是有一些需求和操作并不适合使用java,所以就引入了本地库(Native Libraries)  
	的概念。说白了，就是Hadoop的某些功能，必须通过JNT来协调Java类文件和Native代码生成的库文件一起才能工作。  
	linux系统要运行Native 代码，首先要将Native 编译成目标CPU 架构的[.so]文件。而不同的处理器架构，
	需要编译出相应平台的动态库[.so] 文件，才能被正确的执行，所以最好重新编译一次hadoop源码，
	让[.so]文件与自己处理器相对应。

	【什么是Native Library？】 
	Native Library，一般我们译为本地库或原生库，是由C/C++编写的动态库[.so]，
	并通过JNI(Java Native Interface)机制为java层提供接口。应用一般会出于性能、安全等角度
	考虑将相关逻辑用C/C++实现并编译为库的形式提供接口，供上层或其他模块调用。

### 1. 前期准备工作  

1）CentOS联网   
配置CentOS能连接外网。Linux虚拟机ping www.baidu.com   
+ 注意：采用root角色编译，减少文件夹权限出现问题

2）jar包准备(hadoop源码、JDK8 、 maven、 ant 、protobuf)  
（1）hadoop-2.7.2-src.tar.gz  
（2）jdk-8u144-linux-x64.tar.gz  
（3）apache-ant-1.9.9-bin.tar.gz  
（4）apache-maven-3.0.5-bin.tar.gz  
（5）protobuf-2.5.0.tar.gz  

### 2. jar包安装  
0）注意：所有操作必须在root用户下完成  
1）JDK解压、配置环境变量 JAVA_HOME和PATH，验证java-version(如下都需要验证是否配置成功)  

[root@hadoop101 software] # tar -zxf    jdk-8u144-linux-x64.tar.gz -C /opt/module/  
[root@hadoop101 software]# vi /etc/profile  

#### JAVA_HOME
export JAVA_HOME=/opt/module/jdk1.8.0_144  
export PATH=$PATH:$JAVA_HOME/bin  
[root@hadoop101 software]#source /etc/profile  
验证命令：java -version  
2）Maven解压、配置  MAVEN_HOME和PATH。  
[root@hadoop101 software]# tar -zxvf   apache-maven-3.0.5-bin.tar.gz -C /opt/module/  
[root@hadoop101 apache-maven-3.0.5]#  vi /etc/profile  

#### MAVEN_HOME
export MAVEN_HOME=/opt/module/apache-maven-3.0.5  
export PATH=$PATH:$MAVEN_HOME/bin  
[root@hadoop101 software]#source /etc/profile  
验证命令：mvn -version  
3）ant解压、配置  ANT _HOME和PATH。  
[root@hadoop101 software]# tar -zxvf   apache-ant-1.9.9-bin.tar.gz -C /opt/module/  
[root@hadoop101 apache-ant-1.9.9]# vi /etc/profile  

#### ANT_HOME
export ANT_HOME=/opt/module/apache-ant-1.9.9  
export PATH=$PATH:$ANT_HOME/bin  
[root@hadoop101 software]#source /etc/profile  
验证命令：ant -version  

4）安装  glibc-headers 和  g++  命令如下:   
[root@hadoop101 apache-ant-1.9.9]# yum install   glibc-headers
[root@hadoop101 apache-ant-1.9.9]# yum install gcc-c++  

5）安装make和cmake  
[root@hadoop101 apache-ant-1.9.9]# yum install make  
[root@hadoop101 apache-ant-1.9.9]# yum install cmake  

6）解压protobuf ，进入到解压后protobuf主目录，  /opt/module/protobuf-2.5.0  

然后相继执行命令：  
[root@hadoop101 software]# tar -zxvf protobuf-2.5.0.tar.gz -C /opt/module/  
[root@hadoop101 opt]# cd /opt/module/protobuf-2.5.0/  
[root@hadoop101 protobuf-2.5.0]#./configure   
[root@hadoop101 protobuf-2.5.0]# make   
[root@hadoop101 protobuf-2.5.0]# make check   
[root@hadoop101 protobuf-2.5.0]# make install   
[root@hadoop101 protobuf-2.5.0]# ldconfig   
[root@hadoop101 hadoop-dist]# vi /etc/profile  

#### LD_LIBRARY_PATH
export LD_LIBRARY_PATH=/opt/module/protobuf-2.5.0  
export PATH=$PATH:$LD_LIBRARY_PATH  
[root@hadoop101 software]#source /etc/profile  
验证命令：protoc --version  

7）安装openssl库  
[root@hadoop101 software]#yum install openssl-devel  

8）安装 ncurses-devel库：  
[root@hadoop101 software]#yum install ncurses-devel  
到此，编译工具安装基本完成。  

### 3. 编译源码  
1）解压源码到/opt/目录  
[root@hadoop101 software]# tar -zxvf   hadoop-2.7.2-src.tar.gz -C /opt/  
2）进入到hadoop源码主目录  
[root@hadoop101 hadoop-2.7.2-src]# pwd
/opt/hadoop-2.7.2-src  

3）通过maven执行编译命令  
[root@hadoop101 hadoop-2.7.2-src]#mvn package -Pdist,native -DskipTests -Dtar  
等待时间30分钟左右，最终成功是全部SUCCESS。
 
4）成功的64位hadoop包在/opt/hadoop-2.7.2-src/hadoop-dist/target下。
[root@hadoop101 target]# pwd
/opt/hadoop-2.7.2-src/hadoop-dist/target

### 4. 常见的问题及解决方案  
1）MAVEN install时候JVM内存溢出  
处理方式：在环境配置文件和maven的执行文件均可调整MAVEN_OPT的heap大小。    
（详情查阅MAVEN 编译 JVM调优问题，如：  http://outofmemory.cn/code-snippet/12652/maven-outofmemoryerror-method）

2）编译期间maven报错。可能网络阻塞问题导致依赖库下载不完整导致，多次执行命令（一次通过比较难）：  
[root@hadoop101 hadoop-2.7.2-src]#mvn package -Pdist,native -DskipTests -Dtar

3）报ant、protobuf等错误，插件下载未完整或者插件版本问题，最开始链接有较多特殊情况，同时推荐2.7.0版本的问题汇总帖子   http://www.tuicool.com/articles/IBn63qf
